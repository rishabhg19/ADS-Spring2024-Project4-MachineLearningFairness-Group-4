{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lPkw2MjSwrDI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.metrics import mutual_info_score\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the data by encoding categorical variables and removing null values.\\"
      ],
      "metadata": {
        "id": "fIYPDkH5wzIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv')\n",
        "df['score_text'] = df['score_text'].replace({'High': 2, 'Medium': 1, 'Low': 0})\n",
        "df['score_text'] = df['score_text'].fillna(0)\n",
        "df['v_score_text'] = df['v_score_text'].replace({'High': 2, 'Medium': 1, 'Low': 0})\n",
        "df['v_score_text'] = df['v_score_text'].fillna(0)\n",
        "df = df[df['race'].isin(['Caucasian', 'African-American'])]\n",
        "df['race'] = df['race'].replace({'Caucasian': 1, 'African-American': 0})\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4y8ehHdwzmZ",
        "outputId": "93653802-02e4-4ed3-9b7c-abfae4e9ac71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-09c551943218>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['race'] = df['race'].replace({'Caucasian': 1, 'African-American': 0})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'name', 'first', 'last', 'compas_screening_date', 'sex', 'dob',\n",
              "       'age', 'age_cat', 'race', 'juv_fel_count', 'decile_score',\n",
              "       'juv_misd_count', 'juv_other_count', 'priors_count',\n",
              "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
              "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas',\n",
              "       'c_charge_degree', 'c_charge_desc', 'is_recid', 'r_case_number',\n",
              "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
              "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
              "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
              "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
              "       'decile_score.1', 'score_text', 'screening_date',\n",
              "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
              "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
              "       'start', 'end', 'event', 'two_year_recid'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Make a Logistic Regression model that predicts two_year_recidivism without any prejudice remover. Notice that without any prejudice removal, the model predicts recidivism more accurately for Caucasians compared to African Americans."
      ],
      "metadata": {
        "id": "i2E7KzsYw4QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['id', 'age', 'juv_fel_count', 'juv_misd_count', 'is_recid', 'decile_score','juv_other_count', 'priors_count', 'v_score_text', 'is_violent_recid', 'race']].copy()\n",
        "X = X.fillna(0)\n",
        "Y = df['two_year_recid'].copy()\n",
        "unfair_model = LogisticRegression(max_iter=1000)\n",
        "unfair_model.fit(X, Y)\n",
        "\n",
        "priv = X[X['race'] == 1]\n",
        "unpriv = X[X['race'] == 0]\n",
        "\n",
        "priv_pred = unfair_model.predict(priv)\n",
        "unpriv_pred = unfair_model.predict(unpriv)\n",
        "\n",
        "accuracy_priv = accuracy_score(Y[X['race'] == 1], priv_pred)\n",
        "accuracy_unpriv = accuracy_score(Y[X['race'] == 0], unpriv_pred)\n",
        "\n",
        "print(\"Accuracy for privileged group (race == 1):\", accuracy_priv)\n",
        "print(\"Accuracy for unprivileged group (race == 0):\", accuracy_unpriv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G1G_AMRw1nG",
        "outputId": "8a7d30e9-e352-42ec-cf8b-b49dbebd9088"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for privileged group (race == 1): 0.9759576202118989\n",
            "Accuracy for unprivileged group (race == 0): 0.963474025974026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Store the class Y, the non-sensitive features X, and the sensitive feature S separately."
      ],
      "metadata": {
        "id": "F0sjd5aUw7v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_sensitive_features = ['id', 'age', 'juv_fel_count', 'juv_other_count', 'priors_count', 'v_score_text', 'is_violent_recid']\n",
        "X = df[non_sensitive_features].copy()\n",
        "Y = df['two_year_recid'].copy()\n",
        "S = df['race'].copy()\n",
        "print(X)\n",
        "print(Y)\n",
        "print(S)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXhAQxpJw9JC",
        "outputId": "03d3c1c3-a4cd-45fc-8d21-3d8e620fc2f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         id  age  juv_fel_count  juv_other_count  priors_count  v_score_text  \\\n",
            "1         3   34              0                0             0             0   \n",
            "2         4   24              0                1             4             0   \n",
            "3         5   23              0                0             1             1   \n",
            "6         8   41              0                0            14             0   \n",
            "8        10   39              0                0             0             0   \n",
            "...     ...  ...            ...              ...           ...           ...   \n",
            "7207  10994   30              0                0             0             0   \n",
            "7208  10995   20              0                0             0             2   \n",
            "7209  10996   23              0                0             0             1   \n",
            "7210  10997   23              0                0             0             1   \n",
            "7212  11000   33              0                0             3             0   \n",
            "\n",
            "      is_violent_recid  \n",
            "1                    1  \n",
            "2                    0  \n",
            "3                    0  \n",
            "6                    0  \n",
            "8                    0  \n",
            "...                ...  \n",
            "7207                 0  \n",
            "7208                 0  \n",
            "7209                 0  \n",
            "7210                 0  \n",
            "7212                 0  \n",
            "\n",
            "[6150 rows x 7 columns]\n",
            "1       1\n",
            "2       1\n",
            "3       0\n",
            "6       1\n",
            "8       0\n",
            "       ..\n",
            "7207    1\n",
            "7208    0\n",
            "7209    0\n",
            "7210    0\n",
            "7212    0\n",
            "Name: two_year_recid, Length: 6150, dtype: int64\n",
            "1       0\n",
            "2       0\n",
            "3       0\n",
            "6       1\n",
            "8       1\n",
            "       ..\n",
            "7207    0\n",
            "7208    0\n",
            "7209    0\n",
            "7210    0\n",
            "7212    0\n",
            "Name: race, Length: 6150, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def PRLOSS(unpriv, priv, eta):\n",
        "    unpriv_float = tf.cast(unpriv, dtype=tf.float32)\n",
        "    priv_float = tf.cast(priv, dtype=tf.float32)\n",
        "\n",
        "    n_unpriv = tf.cast(tf.shape(unpriv_float)[0], dtype=tf.float32)\n",
        "    n_priv = tf.cast(tf.shape(priv_float)[0], dtype=tf.float32)\n",
        "\n",
        "    n_unpriv = tf.maximum(n_unpriv, 1.0)\n",
        "    n_priv = tf.maximum(n_priv, 1.0)\n",
        "\n",
        "    Dxisi = tf.stack([n_priv, n_unpriv], axis=0)\n",
        "\n",
        "    y_pred_priv = tf.reduce_sum(priv_float)\n",
        "    y_pred_unpriv = tf.reduce_sum(unpriv_float)\n",
        "\n",
        "    P_ys_stacked = tf.stack([y_pred_priv, y_pred_unpriv], axis=0)\n",
        "    P_ys = P_ys_stacked / Dxisi\n",
        "\n",
        "    P = tf.concat([unpriv_float, priv_float], axis=0)\n",
        "\n",
        "    P_sum = tf.reduce_sum(P)\n",
        "    total_samples = tf.cast(tf.size(unpriv_float) + tf.size(priv_float), dtype=tf.float32)\n",
        "    P_y = P_sum / total_samples\n",
        "\n",
        "    P_y = tf.maximum(P_y, 1e-12)\n",
        "\n",
        "    log_P_ys_1 = tf.math.log(P_ys[1])\n",
        "    log_P_y = tf.math.log(P_y)\n",
        "    P_s1y1 = log_P_ys_1 - log_P_y\n",
        "\n",
        "    log_1_minus_P_ys_1 = tf.math.log(1 - P_ys[1])\n",
        "    log_1_minus_P_y = tf.math.log(1 - P_y)\n",
        "    P_s1y0 = log_1_minus_P_ys_1 - log_1_minus_P_y\n",
        "\n",
        "    log_P_ys_0 = tf.math.log(P_ys[0])\n",
        "    log_P_y = tf.math.log(P_y)\n",
        "    P_s0y1 = log_P_ys_0 - log_P_y\n",
        "\n",
        "    log_1_minus_P_ys_0 = tf.math.log(1 - P_ys[0])\n",
        "    log_1_minus_P_y = tf.math.log(1 - P_y)\n",
        "    P_s0y0 = log_1_minus_P_ys_0 - log_1_minus_P_y\n",
        "\n",
        "    P_s1y1 = tf.reshape(P_s1y1, [-1])\n",
        "    P_s1y0 = tf.reshape(P_s1y0, [-1])\n",
        "    P_s0y1 = tf.reshape(P_s0y1, [-1])\n",
        "    P_s0y0 = tf.reshape(P_s0y0, [-1])\n",
        "\n",
        "    PI_s1y1 = unpriv_float * P_s1y1\n",
        "    PI_s1y0 = (1 - unpriv_float) * P_s1y0\n",
        "    PI_s0y1 = priv_float * P_s0y1\n",
        "    PI_s0y0 = (1 - priv_float) * P_s0y0\n",
        "\n",
        "    PI = tf.reduce_sum(PI_s1y1) + tf.reduce_sum(PI_s1y0) + tf.reduce_sum(PI_s0y1) + tf.reduce_sum(PI_s0y0)\n",
        "\n",
        "    return eta * PI\n",
        "\n",
        "\n",
        "X_train, X_test, S_train, S_test, Y_train, Y_test = train_test_split(X,Y,S, test_size=0.1, random_state=42)\n",
        "\n",
        "# In Rishabh's code the loss and val_loss were NaN during training,\n",
        "# so I checked for NaN or infinite values in the data (to ensure data integrity)\n",
        "print(\"NaN values in X_train:\", np.any(np.isnan(X_train)))\n",
        "print(\"NaN values in X_test:\", np.any(np.isnan(X_test)))\n",
        "print(\"NaN values in Y_train:\", np.any(np.isnan(Y_train)))\n",
        "print(\"NaN values in Y_test:\", np.any(np.isnan(Y_test)))\n",
        "\n",
        "# Normalize the input features -> zero mean and unit variance\n",
        "X_train_normalized = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)\n",
        "X_test_normalized = (X_test - X_test.mean(axis=0)) / X_test.std(axis=0)\n",
        "\n",
        "\n",
        "def prediction_model(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(input_shape,))\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model with the custom loss function\n",
        "model = prediction_model(X_train.shape[1])\n",
        "model.compile(optimizer='adam', loss=lambda y_true, y_pred: PRLOSS(y_true, y_pred, eta))\n",
        "\n",
        "# Train the model with normalized data\n",
        "model.fit(X_train_normalized, Y_train, epochs=10, batch_size=32, validation_data=(X_test_normalized, Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWWKa2YnxOkR",
        "outputId": "14a51ffe-7c08-412b-9cbc-0708933dcad4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN values in X_train: False\n",
            "NaN values in X_test: False\n",
            "NaN values in Y_train: False\n",
            "NaN values in Y_test: False\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 2s 3ms/step - loss: 0.0061 - val_loss: 0.0058\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0047\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0042\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0040\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.0038\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0038\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.0037\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0037\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.0037\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0037\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a58467f34f0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CNClpQT5zizR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}